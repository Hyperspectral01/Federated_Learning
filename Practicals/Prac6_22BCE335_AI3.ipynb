{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cf0e466",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-07T16:00:41.109007Z",
     "iopub.status.busy": "2025-04-07T16:00:41.108783Z",
     "iopub.status.idle": "2025-04-07T16:00:45.283979Z",
     "shell.execute_reply": "2025-04-07T16:00:45.283012Z"
    },
    "papermill": {
     "duration": 4.179887,
     "end_time": "2025-04-07T16:00:45.285254",
     "exception": false,
     "start_time": "2025-04-07T16:00:41.105367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\r\n",
      "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.2->opencv-python) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.2->opencv-python) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.2->opencv-python) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.21.2->opencv-python) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.21.2->opencv-python) (2024.2.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbea30c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T16:00:45.291197Z",
     "iopub.status.busy": "2025-04-07T16:00:45.290967Z",
     "iopub.status.idle": "2025-04-07T16:00:48.500878Z",
     "shell.execute_reply": "2025-04-07T16:00:48.500020Z"
    },
    "papermill": {
     "duration": 3.214901,
     "end_time": "2025-04-07T16:00:48.503039",
     "exception": false,
     "start_time": "2025-04-07T16:00:45.288138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.10/dist-packages (2.4.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install SimpleITK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d58e7a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T16:00:48.509045Z",
     "iopub.status.busy": "2025-04-07T16:00:48.508824Z",
     "iopub.status.idle": "2025-04-07T16:00:51.747356Z",
     "shell.execute_reply": "2025-04-07T16:00:51.746525Z"
    },
    "papermill": {
     "duration": 3.243219,
     "end_time": "2025-04-07T16:00:51.748975",
     "exception": false,
     "start_time": "2025-04-07T16:00:48.505756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\r\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.3,>=1.22.4->scipy) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.3,>=1.22.4->scipy) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.3,>=1.22.4->scipy) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.3,>=1.22.4->scipy) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.3,>=1.22.4->scipy) (2024.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ca19695",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T16:00:51.755309Z",
     "iopub.status.busy": "2025-04-07T16:00:51.755077Z",
     "iopub.status.idle": "2025-04-07T16:01:01.789027Z",
     "shell.execute_reply": "2025-04-07T16:01:01.788239Z"
    },
    "papermill": {
     "duration": 10.038874,
     "end_time": "2025-04-07T16:01:01.790628",
     "exception": false,
     "start_time": "2025-04-07T16:00:51.751754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "from torchvision import transforms\n",
    "\n",
    "import math, random\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import SimpleITK as sitk\n",
    "from scipy.ndimage import label, find_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ba09070",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T16:01:01.797085Z",
     "iopub.status.busy": "2025-04-07T16:01:01.796724Z",
     "iopub.status.idle": "2025-04-07T16:01:01.804187Z",
     "shell.execute_reply": "2025-04-07T16:01:01.803606Z"
    },
    "papermill": {
     "duration": 0.011849,
     "end_time": "2025-04-07T16:01:01.805409",
     "exception": false,
     "start_time": "2025-04-07T16:01:01.793560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class sample_CNN_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(sample_CNN_model,self).__init__()\n",
    "        # Convolutional Layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))  # Replaces conv4\n",
    "        # Fully Connected Layers\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(128, 512)\n",
    "        self.fc2 = nn.Linear(512, 4)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # Output: 160x160\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # Output: 80x80\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # Output: 40x40\n",
    "        x = self.adaptive_pool(x)  # Output: 1x1x128\n",
    "        x = x.view(x.size(0), -1)  # Flatten to 128\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Alternative weight initialization with custom bias values\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # Using Xavier/Glorot uniform initialization instead of Kaiming\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    # Initializing bias with a non-zero value\n",
    "                    nn.init.constant_(m.bias, 0.2)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                # Increased bias for batch norm\n",
    "                nn.init.constant_(m.bias, 0.2)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                # Using Xavier/Glorot normal initialization\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                # Higher bias for fully connected layers\n",
    "                nn.init.constant_(m.bias, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "870ed8a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T16:01:01.811140Z",
     "iopub.status.busy": "2025-04-07T16:01:01.810937Z",
     "iopub.status.idle": "2025-04-07T16:01:02.433499Z",
     "shell.execute_reply": "2025-04-07T16:01:02.432474Z"
    },
    "papermill": {
     "duration": 0.627047,
     "end_time": "2025-04-07T16:01:02.434991",
     "exception": false,
     "start_time": "2025-04-07T16:01:01.807944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4])\n"
     ]
    }
   ],
   "source": [
    "model=sample_CNN_model()\n",
    "out=model(torch.randn((5,3,320,320)))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86eaab2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T16:01:02.441610Z",
     "iopub.status.busy": "2025-04-07T16:01:02.441319Z",
     "iopub.status.idle": "2025-04-07T16:01:02.452862Z",
     "shell.execute_reply": "2025-04-07T16:01:02.452008Z"
    },
    "papermill": {
     "duration": 0.016184,
     "end_time": "2025-04-07T16:01:02.454079",
     "exception": false,
     "start_time": "2025-04-07T16:01:02.437895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class custom_dataset():\n",
    "    def __init__(self,folder_path=None,transform=None):\n",
    "        self.image_paths=[]\n",
    "        self.output_classes=[]\n",
    "        self.transform=transform\n",
    "        if (folder_path!=None):\n",
    "            for categories in os.listdir(folder_path):\n",
    "              for image_name in os.listdir(os.path.join(folder_path,categories)):\n",
    "                self.image_paths.append(os.path.join(folder_path,categories,image_name))\n",
    "                if (categories==\"Diverticulosis\"):\n",
    "                  self.output_classes.append(0)\n",
    "                elif (categories==\"Neoplasm\"):\n",
    "                  self.output_classes.append(1)\n",
    "                elif (categories==\"Peritonitis\"):\n",
    "                  self.output_classes.append(2)\n",
    "                else:\n",
    "                  self.output_classes.append(3)\n",
    "\n",
    "            # # Zip the lists together and convert to a list of pairs\n",
    "            # combined = list(zip(self.image_paths, self.output_classes))\n",
    "\n",
    "            # # Shuffle the combined list\n",
    "            # random.shuffle(combined)\n",
    "\n",
    "            # # Unzip the combined s back into two lists\n",
    "            # self.image_paths, self.output_classes = zip(*combined)\n",
    "\n",
    "            # # Convert back to list type\n",
    "            # self.image_paths = list(self.image_paths)\n",
    "            # self.output_classes = list(self.output_classes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "\n",
    "    # def bias_field_correction(self, channel_np):\n",
    "    #     \"\"\"\n",
    "    #     Apply bias field correction to a single-channel image.\n",
    "    #     \"\"\"\n",
    "    #     sitk_image = sitk.GetImageFromArray(channel_np)\n",
    "    #     # Create a mask using Otsu thresholding\n",
    "    #     mask = sitk.OtsuThreshold(sitk_image, 0, 1, 200)\n",
    "    #     corrected = sitk.N4BiasFieldCorrection(sitk_image, mask)\n",
    "    #     corrected_np = sitk.GetArrayFromImage(corrected)\n",
    "    #     return corrected_np\n",
    "\n",
    "    # def crop_to_white_blob(self, image_np):\n",
    "    #     \"\"\"\n",
    "    #     Crop the image to the bounding box of the connected component \n",
    "    #     (from the white blob) that contains the center of the image.\n",
    "    #     \"\"\"\n",
    "    #     # Convert first three channels (RGB) to grayscale by averaging\n",
    "    #     gray = np.mean(image_np[:, :, :3], axis=2)\n",
    "    #     # Create a binary mask; adjust threshold as needed for your data\n",
    "    #     binary = gray > 200\n",
    "\n",
    "    #     # Label connected regions and find bounding boxes\n",
    "    #     labeled, num_features = label(binary)\n",
    "    #     center_y, center_x = gray.shape[0] // 2, gray.shape[1] // 2\n",
    "    #     selected_bbox = None\n",
    "    #     objects = find_objects(labeled)\n",
    "    #     for i, slice_tuple in enumerate(objects, start=1):\n",
    "    #         y_slice, x_slice = slice_tuple\n",
    "    #         # Check if the image center lies within the component's bounding box\n",
    "    #         if (y_slice.start <= center_y < y_slice.stop) and (x_slice.start <= center_x < x_slice.stop):\n",
    "    #             selected_bbox = slice_tuple\n",
    "    #             break\n",
    "\n",
    "    #     if selected_bbox is not None:\n",
    "    #         y_slice, x_slice = selected_bbox\n",
    "    #         cropped = image_np[y_slice, x_slice, :]\n",
    "    #         return cropped\n",
    "    #     else:\n",
    "    #         # If no white blob is detected near the center, return the original image\n",
    "    #         return image_np\n",
    "\n",
    "    # def zscore_normalize(self, image_np):\n",
    "    #     \"\"\"\n",
    "    #     Apply per-channel z-score normalization.\n",
    "    #     \"\"\"\n",
    "    #     # Process each channel independently\n",
    "    #     for c in range(image_np.shape[2]):\n",
    "    #         channel = image_np[:, :, c]\n",
    "    #         mean = channel.mean()\n",
    "    #         std = channel.std()\n",
    "    #         if std > 0:\n",
    "    #             image_np[:, :, c] = (channel - mean) / std\n",
    "    #         else:\n",
    "    #             image_np[:, :, c] = channel - mean\n",
    "    #     return image_np\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the preprocessed image (assumed to already have the desired size and channels)\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        \n",
    "        # Optionally apply additional transformations if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            # Convert image to a numpy array and then to a torch tensor with shape (C, H, W)\n",
    "            image_np = np.array(image)\n",
    "            image = torch.from_numpy(image_np.transpose(2, 0, 1))\n",
    "        \n",
    "        return image, self.output_classes[idx]\n",
    "\n",
    "\n",
    "    def take_out_items(self, ratio):\n",
    "\n",
    "        indices_label0 = [i for i, label in enumerate(self.output_classes) if label == 0]\n",
    "        indices_label1 = [i for i, label in enumerate(self.output_classes) if label == 1]\n",
    "        indices_label2 = [i for i, label in enumerate(self.output_classes) if label == 2]\n",
    "        indices_label3 = [i for i, label in enumerate(self.output_classes) if label == 3]\n",
    "\n",
    "        num_to_remove_0 = math.floor(len(indices_label0) * ratio)\n",
    "        num_to_remove_1 = math.floor(len(indices_label1) * ratio)\n",
    "        num_to_remove_2 = math.floor(len(indices_label2) * ratio)\n",
    "        num_to_remove_3 = math.floor(len(indices_label3) * ratio)\n",
    "\n",
    "        selected_indices_0 = random.sample(indices_label0, num_to_remove_0) if num_to_remove_0 > 0 else []\n",
    "        selected_indices_1 = random.sample(indices_label1, num_to_remove_1) if num_to_remove_1 > 0 else []\n",
    "        selected_indices_2 = random.sample(indices_label2, num_to_remove_2) if num_to_remove_2 > 0 else []\n",
    "        selected_indices_3 = random.sample(indices_label3, num_to_remove_3) if num_to_remove_3 > 0 else []\n",
    "\n",
    "        selected_indices = set(selected_indices_0 + selected_indices_1 + selected_indices_2 + selected_indices_3)\n",
    "\n",
    "        removed_image_paths = [self.image_paths[i] for i in selected_indices]\n",
    "        removed_labels = [self.output_classes[i] for i in selected_indices]\n",
    "\n",
    "        new_image_paths = []\n",
    "        new_output_classes = []\n",
    "        for idx, (img_path, label) in enumerate(zip(self.image_paths, self.output_classes)):\n",
    "            if idx not in selected_indices:\n",
    "                new_image_paths.append(img_path)\n",
    "                new_output_classes.append(label)\n",
    "\n",
    "        self.image_paths = new_image_paths\n",
    "        self.output_classes = new_output_classes\n",
    "\n",
    "        return removed_image_paths, removed_labels\n",
    "\n",
    "    def add_items(self, list_of_image_paths,list_of_labels):\n",
    "        self.image_paths.extend(list_of_image_paths)\n",
    "        self.output_classes.extend(list_of_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08fa5840",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T16:01:02.460142Z",
     "iopub.status.busy": "2025-04-07T16:01:02.459942Z",
     "iopub.status.idle": "2025-04-07T21:58:40.855055Z",
     "shell.execute_reply": "2025-04-07T21:58:40.854328Z"
    },
    "papermill": {
     "duration": 21458.399914,
     "end_time": "2025-04-07T21:58:40.856670",
     "exception": false,
     "start_time": "2025-04-07T16:01:02.456756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Training with: Test Ratio=0.2, Batch Size=100, Rounds=10, Epochs=2, LR=0.001 ===\n",
      "\n",
      "Round 1/10:\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4315711855888367\n",
      "Accuracy:  0.22625\n",
      "precision:  0.06967112904211749\n",
      "recall:  0.22625\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4314945340156555\n",
      "Accuracy:  0.2275\n",
      "precision:  0.07051282051282051\n",
      "recall:  0.2275\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4313666969537735\n",
      "Accuracy:  0.2275\n",
      "precision:  0.07051282051282051\n",
      "recall:  0.2275\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.431649848818779\n",
      "Accuracy:  0.2275\n",
      "precision:  0.07051282051282051\n",
      "recall:  0.2275\n",
      "############## GLOBAL MODEL LOGS ##############\n",
      "  Round 1 Metrics - Loss: 1.4285 | Accuracy: 0.2500 | Precision: 0.0625 | Recall: 0.2500 | F1: 0.1000       \n",
      "\n",
      "Round 2/10:\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4286398589611053\n",
      "Accuracy:  0.21625\n",
      "precision:  0.0712890625\n",
      "recall:  0.21625\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4284072518348694\n",
      "Accuracy:  0.21875\n",
      "precision:  0.07283549783549784\n",
      "recall:  0.21875\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4284681528806686\n",
      "Accuracy:  0.21875\n",
      "precision:  0.07283549783549784\n",
      "recall:  0.21875\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4287443906068802\n",
      "Accuracy:  0.22125\n",
      "precision:  0.0745281273131014\n",
      "recall:  0.22125\n",
      "############## GLOBAL MODEL LOGS ##############\n",
      "  Round 2 Metrics - Loss: 1.4263 | Accuracy: 0.2500 | Precision: 0.0625 | Recall: 0.2500 | F1: 0.1000       \n",
      "\n",
      "Round 3/10:\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.425682619214058\n",
      "Accuracy:  0.205\n",
      "precision:  0.0730123825231027\n",
      "recall:  0.20500000000000002\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.425603672862053\n",
      "Accuracy:  0.205\n",
      "precision:  0.06941030205419457\n",
      "recall:  0.20500000000000002\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4255698025226593\n",
      "Accuracy:  0.205\n",
      "precision:  0.06968631069080844\n",
      "recall:  0.20500000000000002\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4257242232561111\n",
      "Accuracy:  0.20625\n",
      "precision:  0.06545881846947867\n",
      "recall:  0.20625\n",
      "############## GLOBAL MODEL LOGS ##############\n",
      "  Round 3 Metrics - Loss: 1.4243 | Accuracy: 0.2500 | Precision: 0.0625 | Recall: 0.2500 | F1: 0.1000       \n",
      "\n",
      "Round 4/10:\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4231389909982681\n",
      "Accuracy:  0.1925\n",
      "precision:  0.06870302749280319\n",
      "recall:  0.1925\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.422613948583603\n",
      "Accuracy:  0.195\n",
      "precision:  0.06692263231772769\n",
      "recall:  0.195\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4227191507816315\n",
      "Accuracy:  0.195\n",
      "precision:  0.06692263231772769\n",
      "recall:  0.195\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4230223149061203\n",
      "Accuracy:  0.195\n",
      "precision:  0.06692263231772769\n",
      "recall:  0.195\n",
      "############## GLOBAL MODEL LOGS ##############\n",
      "  Round 4 Metrics - Loss: 1.4223 | Accuracy: 0.2500 | Precision: 0.0625 | Recall: 0.2500 | F1: 0.1000       \n",
      "\n",
      "Round 5/10:\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4203501045703888\n",
      "Accuracy:  0.18375\n",
      "precision:  0.0669477581242287\n",
      "recall:  0.18375\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4199353754520416\n",
      "Accuracy:  0.18625\n",
      "precision:  0.0679328191426795\n",
      "recall:  0.18625\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4200590550899506\n",
      "Accuracy:  0.185\n",
      "precision:  0.06743814844373504\n",
      "recall:  0.185\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.420210674405098\n",
      "Accuracy:  0.18875\n",
      "precision:  0.06893576469376192\n",
      "recall:  0.18875\n",
      "############## GLOBAL MODEL LOGS ##############\n",
      "  Round 5 Metrics - Loss: 1.4205 | Accuracy: 0.2500 | Precision: 0.0625 | Recall: 0.2500 | F1: 0.1000       \n",
      "\n",
      "Round 6/10:\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.41765396296978\n",
      "Accuracy:  0.17375\n",
      "precision:  0.07755555555555556\n",
      "recall:  0.17375\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4172757714986801\n",
      "Accuracy:  0.18375\n",
      "precision:  0.0789312707302485\n",
      "recall:  0.18375000000000002\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4174237102270126\n",
      "Accuracy:  0.1825\n",
      "precision:  0.07697833775419982\n",
      "recall:  0.1825\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4177099466323853\n",
      "Accuracy:  0.185\n",
      "precision:  0.07812236642508005\n",
      "recall:  0.185\n",
      "############## GLOBAL MODEL LOGS ##############\n",
      "  Round 6 Metrics - Loss: 1.4188 | Accuracy: 0.2500 | Precision: 0.0625 | Recall: 0.2500 | F1: 0.1000       \n",
      "\n",
      "Round 7/10:\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.415037989616394\n",
      "Accuracy:  0.15125\n",
      "precision:  0.07245167970547631\n",
      "recall:  0.15125\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4149262607097626\n",
      "Accuracy:  0.15875\n",
      "precision:  0.0752719944237355\n",
      "recall:  0.15875\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.415024220943451\n",
      "Accuracy:  0.15625\n",
      "precision:  0.07340399610136453\n",
      "recall:  0.15625\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4150761514902115\n",
      "Accuracy:  0.1625\n",
      "precision:  0.07440351273689988\n",
      "recall:  0.1625\n",
      "############## GLOBAL MODEL LOGS ##############\n",
      "  Round 7 Metrics - Loss: 1.4172 | Accuracy: 0.2500 | Precision: 0.0625 | Recall: 0.2500 | F1: 0.1000       \n",
      "\n",
      "Round 8/10:\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4126055836677551\n",
      "Accuracy:  0.145\n",
      "precision:  0.0729076790336497\n",
      "recall:  0.14500000000000002\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.41244238615036\n",
      "Accuracy:  0.1475\n",
      "precision:  0.07148921565085443\n",
      "recall:  0.1475\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4125143587589264\n",
      "Accuracy:  0.1475\n",
      "precision:  0.07111578209844105\n",
      "recall:  0.1475\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4128092229366302\n",
      "Accuracy:  0.1475\n",
      "precision:  0.07138407757602805\n",
      "recall:  0.1475\n",
      "############## GLOBAL MODEL LOGS ##############\n",
      "  Round 8 Metrics - Loss: 1.4156 | Accuracy: 0.2500 | Precision: 0.0625 | Recall: 0.2500 | F1: 0.1000       \n",
      "\n",
      "Round 9/10:\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4103612154722214\n",
      "Accuracy:  0.1425\n",
      "precision:  0.07547328262590877\n",
      "recall:  0.14250000000000002\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.410105124115944\n",
      "Accuracy:  0.145\n",
      "precision:  0.07660988582141795\n",
      "recall:  0.14500000000000002\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4101871401071548\n",
      "Accuracy:  0.145\n",
      "precision:  0.07675544794188863\n",
      "recall:  0.14500000000000002\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4103381633758545\n",
      "Accuracy:  0.14375\n",
      "precision:  0.0760412655628289\n",
      "recall:  0.14375\n",
      "############## GLOBAL MODEL LOGS ##############\n",
      "  Round 9 Metrics - Loss: 1.4142 | Accuracy: 0.2500 | Precision: 0.0625 | Recall: 0.2500 | F1: 0.1000       \n",
      "\n",
      "Round 10/10:\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4079429358243942\n",
      "Accuracy:  0.13375\n",
      "precision:  0.07284847063172187\n",
      "recall:  0.13374999999999998\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4077770411968231\n",
      "Accuracy:  0.14\n",
      "precision:  0.07545454545454545\n",
      "recall:  0.14\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4079494029283524\n",
      "Accuracy:  0.13875\n",
      "precision:  0.07491455297715416\n",
      "recall:  0.13875\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Computing Local Gradients:\n",
      "avg_loss:  1.4080379903316498\n",
      "Accuracy:  0.14\n",
      "precision:  0.07508255690529544\n",
      "recall:  0.13999999999999999\n",
      "############## GLOBAL MODEL LOGS ##############\n",
      "  Round 10 Metrics - Loss: 1.4128 | Accuracy: 0.2500 | Precision: 0.0625 | Recall: 0.2500 | F1: 0.1000       \n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import copy\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ====================== METRICS & PLOTTING ======================\n",
    "def test_global_model(global_model, test_dataloader):\n",
    "    \"\"\"Returns test loss, accuracy, precision, recall, F1\"\"\"\n",
    "    global_model.eval()\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (images, labels) in enumerate(test_dataloader):\n",
    "            outputs = global_model(images)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(test_dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    # print(\"The Accuracy of local model after training: \",accuracy)\n",
    "    return avg_loss, accuracy, precision, recall, f1\n",
    "\n",
    "def plot_learning_curves(metrics_history, hyperparams):\n",
    "    \"\"\"Plots metrics vs communication rounds for a single hyperparameter configuration\"\"\"\n",
    "    rounds = list(range(1, len(metrics_history['loss'])+1))\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.suptitle(f\"Test Ratio: {hyperparams['test_ratio']} | Batch Size: {hyperparams['batch_size']}\\n\"\n",
    "                 f\"Rounds: {hyperparams['num_rounds']} | Epochs: {hyperparams['num_epochs']} | LR: {hyperparams['learning_rate']}\", \n",
    "                 y=1.02)\n",
    "    \n",
    "    metrics = ['loss', 'accuracy', 'precision', 'recall', 'f1']\n",
    "    titles = ['Loss', 'Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    \n",
    "    for i, (metric, title) in enumerate(zip(metrics, titles), 1):\n",
    "        plt.subplot(2, 3, i)\n",
    "        plt.plot(rounds, metrics_history[metric], marker='o')\n",
    "        plt.xlabel('Communication Round')\n",
    "        plt.ylabel(title)\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"/kaggle/working/learning_curves_ratio_{hyperparams['test_ratio']}_bs_{hyperparams['batch_size']}_rounds_{hyperparams['num_rounds']}_epochs_{hyperparams['num_epochs']}_lr_{hyperparams['learning_rate']}.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_hyperparameter_comparisons(all_results):\n",
    "    \"\"\"Generates plots comparing metrics across different hyperparameters\"\"\"\n",
    "    hyperparams = ['test_ratio', 'batch_size', 'num_rounds', 'num_epochs', 'learning_rate']\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1', 'loss']\n",
    "    \n",
    "    for hp in hyperparams:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for metric in metrics:\n",
    "            x = [res['hyperparams'][hp] for res in all_results]\n",
    "            y = [res['final_'+metric] for res in all_results]\n",
    "            plt.scatter(x, y, label=metric, alpha=0.6)\n",
    "        \n",
    "        plt.xlabel(hp)\n",
    "        plt.ylabel('Metric Value')\n",
    "        plt.title(f'Impact of {hp} on Metrics')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f\"/kaggle/working/hyperparam_{hp}_comparison.png\")\n",
    "        plt.close()\n",
    "\n",
    "# ====================== FEDERATED LEARNING CORE (FedSGD) ======================\n",
    "def train_local_model(global_model, num_epochs, train_dataloader, test_dataloader, learning_rate):\n",
    "    \"\"\"\n",
    "    Computes gradients on a copy of the global model using local training data.\n",
    "    Instead of updating local weights, the aggregated gradients are sent back.\n",
    "    \"\"\"\n",
    "    print(\"############### LOCAL MODEL LOGS ############\")\n",
    "    # Create a local copy so that the global model remains unchanged during local gradient computation\n",
    "    local_model = copy.deepcopy(global_model)\n",
    "    local_model.train()\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "    optimizer = torch.optim.Adam(local_model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    \n",
    "    # Use a learning rate scheduler to prevent getting stuck\n",
    "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=0.001, \n",
    "        steps_per_epoch=len(train_dataloader), epochs=num_epochs\n",
    "    )\n",
    "    \n",
    "    aggregated_gradients = {}\n",
    "    total_samples = 0\n",
    "    \n",
    "    # Compute gradients over the local dataset (over num_epochs)\n",
    "    for _ in range(num_epochs):\n",
    "        for images, labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = local_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            batch_size = images.size(0)\n",
    "            total_samples += batch_size\n",
    "            for name, param in local_model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    if name not in aggregated_gradients:\n",
    "                        aggregated_gradients[name] = param.grad.detach().clone() * batch_size\n",
    "                    else:\n",
    "                        aggregated_gradients[name] += param.grad.detach().clone() * batch_size\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(local_model.parameters(), max_norm=1.0)\n",
    "\n",
    "        val_loss, val_acc, _, _, _ = test_global_model(local_model, test_dataloader)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    # Average the gradients over all samples\n",
    "    for name in aggregated_gradients:\n",
    "        aggregated_gradients[name] /= total_samples\n",
    "\n",
    "    # Log local performance on the copied model (without applying the gradient update)\n",
    "    avg_loss, accuracy, precision, recall, f1 = test_global_model(local_model, test_dataloader)\n",
    "    print(\"After Computing Local Gradients:\")\n",
    "    print(\"avg_loss: \", avg_loss)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"precision: \", precision)\n",
    "    print(\"recall: \", recall)\n",
    "    \n",
    "    return aggregated_gradients\n",
    "\n",
    "def federated_learning_algo(model, train_dataloaders, test_dataloader, num_rounds, num_epochs, learning_rate, hyperparams):\n",
    "    global_model = copy.deepcopy(model)\n",
    "    # Calculate total samples across all clients for weighting\n",
    "    total_samples_across_clients = sum([len(dataloader.dataset) for dataloader in train_dataloaders])\n",
    "    metrics_history = {'loss': [], 'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "    \n",
    "    print(f\"\\n=== Starting Training with: Test Ratio={hyperparams['test_ratio']}, Batch Size={hyperparams['batch_size']}, \"\n",
    "          f\"Rounds={num_rounds}, Epochs={num_epochs}, LR={learning_rate} ===\")\n",
    "\n",
    "    # Create a list of client optimizers (used solely for zeroing gradients in local computations)\n",
    "    client_optimizers = [\n",
    "        torch.optim.Adam(global_model.parameters(), lr=learning_rate)\n",
    "        for _ in range(len(train_dataloaders))\n",
    "    ]\n",
    "    \n",
    "    for round in range(1, num_rounds+1):\n",
    "        print(f\"\\nRound {round}/{num_rounds}:\")\n",
    "        \n",
    "        # Local Gradient Computation\n",
    "        client_gradients = []\n",
    "        for client_id, (dataloader, optimizer) in enumerate(zip(train_dataloaders, client_optimizers), 1):\n",
    "            print(f\"  Client {client_id} computing gradients...\", end='\\r')\n",
    "            grads = train_local_model(global_model, num_epochs, dataloader, test_dataloader, learning_rate)\n",
    "            client_gradients.append(grads)\n",
    "        \n",
    "        # Aggregation (Federated Gradient Averaging)\n",
    "        aggregated_gradients = {}\n",
    "        for key in client_gradients[0].keys():\n",
    "            for grads, dataloader in zip(client_gradients, train_dataloaders):\n",
    "                weight = len(dataloader.dataset) / total_samples_across_clients\n",
    "                if key not in aggregated_gradients:\n",
    "                    aggregated_gradients[key] = weight * grads[key]\n",
    "                else:\n",
    "                    aggregated_gradients[key] += weight * grads[key]\n",
    "                    \n",
    "        # Global update using the aggregated gradients (FedSGD update)\n",
    "        with torch.no_grad():\n",
    "            for name, param in global_model.named_parameters():\n",
    "                if name in aggregated_gradients:\n",
    "                    param.data = param.data - learning_rate * aggregated_gradients[name]\n",
    "        \n",
    "        # Testing\n",
    "        test_loss, acc, prec, rec, f1 = test_global_model(global_model, test_dataloader)\n",
    "        metrics_history['loss'].append(test_loss)\n",
    "        metrics_history['accuracy'].append(acc)\n",
    "        metrics_history['precision'].append(prec)\n",
    "        metrics_history['recall'].append(rec)\n",
    "        metrics_history['f1'].append(f1)\n",
    "\n",
    "        print(\"############## GLOBAL MODEL LOGS ##############\")\n",
    "        print(f\"  Round {round} Metrics - Loss: {test_loss:.4f} | Accuracy: {acc:.4f} | \"\n",
    "              f\"Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}       \")\n",
    "    \n",
    "    # Save learning curves for this configuration\n",
    "    plot_learning_curves(metrics_history, hyperparams)\n",
    "\n",
    "    torch.save(global_model, \"/kaggle/working/\" + \n",
    "               f\"{hyperparams['test_ratio']}_{hyperparams['batch_size']}_{hyperparams['num_rounds']}_{hyperparams['num_epochs']}_{hyperparams['learning_rate']}.pth\")\n",
    "    torch.save(global_model.state_dict(), \"/kaggle/working/\" + \n",
    "               f\"{hyperparams['test_ratio']}_{hyperparams['batch_size']}_{hyperparams['num_rounds']}_{hyperparams['num_epochs']}_{hyperparams['learning_rate']}_state_dict.pth\")\n",
    "    \n",
    "    # Return final metrics for hyperparameter comparison\n",
    "    return {\n",
    "        'hyperparams': hyperparams,\n",
    "        'final_accuracy': metrics_history['accuracy'][-1],\n",
    "        'final_precision': metrics_history['precision'][-1],\n",
    "        'final_recall': metrics_history['recall'][-1],\n",
    "        'final_f1': metrics_history['f1'][-1],\n",
    "        'final_loss': metrics_history['loss'][-1]\n",
    "    }\n",
    "\n",
    "# ====================== MAIN EXECUTION ======================\n",
    "\n",
    "# Assume model and datasets are predefined\n",
    "model = sample_CNN_model()  # Your model definition\n",
    "\n",
    "# For training data (with augmentations)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((320, 320)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# For validation/test data (no augmentations)\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((320, 320)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "center_datasets = [custom_dataset(folder_path=\"/kaggle/input/new-centers/Center_1_zipp/Center_1\",transform=train_transforms),custom_dataset(folder_path=\"/kaggle/input/new-centers/Center_2_zipp/Center_2\",transform=train_transforms), custom_dataset(folder_path=\"/kaggle/input/new-centers/Center_3_zipp/Center_3\",transform=train_transforms), custom_dataset(folder_path=\"/kaggle/input/new-centers/Center_4_zipp/Center_4\",transform=train_transforms)]  # Your central datasets\n",
    "\n",
    "# Hyperparameter Search Space\n",
    "test_ratios = [0.2]\n",
    "batch_sizes = [100]\n",
    "num_rounds_list = [10]\n",
    "num_epochs_list = [2]\n",
    "learning_rates = [0.001]  # ,0.00007,0.0001,0.0007\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for test_ratio in test_ratios:\n",
    "    \n",
    "    # Dataset preparation\n",
    "    to_put_image_paths = []\n",
    "    to_put_image_labels = []\n",
    "    for center_dataset in center_datasets:\n",
    "        to_append_image_paths, to_append_image_labels = center_dataset.take_out_items(test_ratio)\n",
    "        to_put_image_paths.extend(to_append_image_paths)\n",
    "        to_put_image_labels.extend(to_append_image_labels)\n",
    "    test_dataset = custom_dataset(transform=test_transforms)  # Here the folder path is None so that it makes an empty dataset\n",
    "    test_dataset.add_items(to_put_image_paths, to_put_image_labels)\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        train_dataloaders = [DataLoader(center_dataset, batch_size=batch_size, shuffle=True) for center_dataset in center_datasets]\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        for num_rounds in num_rounds_list:\n",
    "            for num_epochs in num_epochs_list:\n",
    "                for lr in learning_rates:\n",
    "                    hyperparams = {\n",
    "                        'test_ratio': test_ratio,\n",
    "                        'batch_size': batch_size,\n",
    "                        'num_rounds': num_rounds,\n",
    "                        'num_epochs': num_epochs,\n",
    "                        'learning_rate': lr\n",
    "                    }\n",
    "                    \n",
    "                    # Run federated learning (FedSGD)\n",
    "                    result = federated_learning_algo(\n",
    "                        model, train_dataloaders, test_dataloader, \n",
    "                        num_rounds, num_epochs, lr, hyperparams\n",
    "                    )\n",
    "                    all_results.append(result)\n",
    "                    \n",
    "\n",
    "# Generate hyperparameter comparison plots\n",
    "plot_hyperparameter_comparisons(all_results)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7046228,
     "sourceId": 11271844,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21484.061618,
   "end_time": "2025-04-07T21:58:42.609985",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-07T16:00:38.548367",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
