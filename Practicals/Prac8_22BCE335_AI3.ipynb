{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df7b6733",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-07T16:12:38.228024Z",
     "iopub.status.busy": "2025-04-07T16:12:38.227754Z",
     "iopub.status.idle": "2025-04-07T16:12:42.292394Z",
     "shell.execute_reply": "2025-04-07T16:12:42.291387Z"
    },
    "papermill": {
     "duration": 4.069805,
     "end_time": "2025-04-07T16:12:42.293906",
     "exception": false,
     "start_time": "2025-04-07T16:12:38.224101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\r\n",
      "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.2->opencv-python) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.2->opencv-python) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.2->opencv-python) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.21.2->opencv-python) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.21.2->opencv-python) (2024.2.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9217ae24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T16:12:42.300375Z",
     "iopub.status.busy": "2025-04-07T16:12:42.300125Z",
     "iopub.status.idle": "2025-04-07T16:12:45.627859Z",
     "shell.execute_reply": "2025-04-07T16:12:45.626938Z"
    },
    "papermill": {
     "duration": 3.332564,
     "end_time": "2025-04-07T16:12:45.629511",
     "exception": false,
     "start_time": "2025-04-07T16:12:42.296947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.10/dist-packages (2.4.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install SimpleITK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc39cd6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T16:12:45.635894Z",
     "iopub.status.busy": "2025-04-07T16:12:45.635659Z",
     "iopub.status.idle": "2025-04-07T16:12:48.890151Z",
     "shell.execute_reply": "2025-04-07T16:12:48.889012Z"
    },
    "papermill": {
     "duration": 3.259434,
     "end_time": "2025-04-07T16:12:48.891890",
     "exception": false,
     "start_time": "2025-04-07T16:12:45.632456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\r\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.3,>=1.22.4->scipy) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.3,>=1.22.4->scipy) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.3,>=1.22.4->scipy) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.3,>=1.22.4->scipy) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.3,>=1.22.4->scipy) (2024.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4b6dedb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T16:12:48.899429Z",
     "iopub.status.busy": "2025-04-07T16:12:48.899147Z",
     "iopub.status.idle": "2025-04-07T16:12:59.132932Z",
     "shell.execute_reply": "2025-04-07T16:12:59.132235Z"
    },
    "papermill": {
     "duration": 10.239384,
     "end_time": "2025-04-07T16:12:59.134508",
     "exception": false,
     "start_time": "2025-04-07T16:12:48.895124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "from torchvision import transforms\n",
    "\n",
    "import math, random\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import SimpleITK as sitk\n",
    "from scipy.ndimage import label, find_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e8de049",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T16:12:59.141400Z",
     "iopub.status.busy": "2025-04-07T16:12:59.141001Z",
     "iopub.status.idle": "2025-04-07T16:12:59.148696Z",
     "shell.execute_reply": "2025-04-07T16:12:59.148003Z"
    },
    "papermill": {
     "duration": 0.012434,
     "end_time": "2025-04-07T16:12:59.149981",
     "exception": false,
     "start_time": "2025-04-07T16:12:59.137547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class sample_CNN_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Convolutional Layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))  # Replaces conv4\n",
    "        # Fully Connected Layers\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(128, 512)\n",
    "        self.fc2 = nn.Linear(512, 4)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # Output: 160x160\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # Output: 80x80\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # Output: 40x40\n",
    "        x = self.adaptive_pool(x)  # Output: 1x1x128\n",
    "        x = x.view(x.size(0), -1)  # Flatten to 128\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Alternative weight initialization with custom bias values\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # Using Xavier/Glorot uniform initialization instead of Kaiming\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    # Initializing bias with a non-zero value\n",
    "                    nn.init.constant_(m.bias, 0.2)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                # Increased bias for batch norm\n",
    "                nn.init.constant_(m.bias, 0.2)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                # Using Xavier/Glorot normal initialization\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                # Higher bias for fully connected layers\n",
    "                nn.init.constant_(m.bias, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bba3dfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T16:12:59.156393Z",
     "iopub.status.busy": "2025-04-07T16:12:59.156148Z",
     "iopub.status.idle": "2025-04-07T16:12:59.752233Z",
     "shell.execute_reply": "2025-04-07T16:12:59.751203Z"
    },
    "papermill": {
     "duration": 0.600807,
     "end_time": "2025-04-07T16:12:59.753720",
     "exception": false,
     "start_time": "2025-04-07T16:12:59.152913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4])\n"
     ]
    }
   ],
   "source": [
    "model=sample_CNN_model()\n",
    "out=model(torch.randn((5,3,320,320)))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8535348",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T16:12:59.760682Z",
     "iopub.status.busy": "2025-04-07T16:12:59.760423Z",
     "iopub.status.idle": "2025-04-07T16:12:59.771766Z",
     "shell.execute_reply": "2025-04-07T16:12:59.770982Z"
    },
    "papermill": {
     "duration": 0.015985,
     "end_time": "2025-04-07T16:12:59.772862",
     "exception": false,
     "start_time": "2025-04-07T16:12:59.756877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class custom_dataset():\n",
    "    def __init__(self,folder_path=None,transform=None):\n",
    "        self.image_paths=[]\n",
    "        self.output_classes=[]\n",
    "        self.transform=transform\n",
    "        if (folder_path!=None):\n",
    "            for categories in os.listdir(folder_path):\n",
    "              for image_name in os.listdir(os.path.join(folder_path,categories)):\n",
    "                self.image_paths.append(os.path.join(folder_path,categories,image_name))\n",
    "                if (categories==\"Diverticulosis\"):\n",
    "                  self.output_classes.append(0)\n",
    "                elif (categories==\"Neoplasm\"):\n",
    "                  self.output_classes.append(1)\n",
    "                elif (categories==\"Peritonitis\"):\n",
    "                  self.output_classes.append(2)\n",
    "                else:\n",
    "                  self.output_classes.append(3)\n",
    "\n",
    "            # # Zip the lists together and convert to a list of pairs\n",
    "            # combined = list(zip(self.image_paths, self.output_classes))\n",
    "\n",
    "            # # Shuffle the combined list\n",
    "            # random.shuffle(combined)\n",
    "\n",
    "            # # Unzip the combined s back into two lists\n",
    "            # self.image_paths, self.output_classes = zip(*combined)\n",
    "\n",
    "            # # Convert back to list type\n",
    "            # self.image_paths = list(self.image_paths)\n",
    "            # self.output_classes = list(self.output_classes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "\n",
    "    # def bias_field_correction(self, channel_np):\n",
    "    #     \"\"\"\n",
    "    #     Apply bias field correction to a single-channel image.\n",
    "    #     \"\"\"\n",
    "    #     sitk_image = sitk.GetImageFromArray(channel_np)\n",
    "    #     # Create a mask using Otsu thresholding\n",
    "    #     mask = sitk.OtsuThreshold(sitk_image, 0, 1, 200)\n",
    "    #     corrected = sitk.N4BiasFieldCorrection(sitk_image, mask)\n",
    "    #     corrected_np = sitk.GetArrayFromImage(corrected)\n",
    "    #     return corrected_np\n",
    "\n",
    "    # def crop_to_white_blob(self, image_np):\n",
    "    #     \"\"\"\n",
    "    #     Crop the image to the bounding box of the connected component \n",
    "    #     (from the white blob) that contains the center of the image.\n",
    "    #     \"\"\"\n",
    "    #     # Convert first three channels (RGB) to grayscale by averaging\n",
    "    #     gray = np.mean(image_np[:, :, :3], axis=2)\n",
    "    #     # Create a binary mask; adjust threshold as needed for your data\n",
    "    #     binary = gray > 200\n",
    "\n",
    "    #     # Label connected regions and find bounding boxes\n",
    "    #     labeled, num_features = label(binary)\n",
    "    #     center_y, center_x = gray.shape[0] // 2, gray.shape[1] // 2\n",
    "    #     selected_bbox = None\n",
    "    #     objects = find_objects(labeled)\n",
    "    #     for i, slice_tuple in enumerate(objects, start=1):\n",
    "    #         y_slice, x_slice = slice_tuple\n",
    "    #         # Check if the image center lies within the component's bounding box\n",
    "    #         if (y_slice.start <= center_y < y_slice.stop) and (x_slice.start <= center_x < x_slice.stop):\n",
    "    #             selected_bbox = slice_tuple\n",
    "    #             break\n",
    "\n",
    "    #     if selected_bbox is not None:\n",
    "    #         y_slice, x_slice = selected_bbox\n",
    "    #         cropped = image_np[y_slice, x_slice, :]\n",
    "    #         return cropped\n",
    "    #     else:\n",
    "    #         # If no white blob is detected near the center, return the original image\n",
    "    #         return image_np\n",
    "\n",
    "    # def zscore_normalize(self, image_np):\n",
    "    #     \"\"\"\n",
    "    #     Apply per-channel z-score normalization.\n",
    "    #     \"\"\"\n",
    "    #     # Process each channel independently\n",
    "    #     for c in range(image_np.shape[2]):\n",
    "    #         channel = image_np[:, :, c]\n",
    "    #         mean = channel.mean()\n",
    "    #         std = channel.std()\n",
    "    #         if std > 0:\n",
    "    #             image_np[:, :, c] = (channel - mean) / std\n",
    "    #         else:\n",
    "    #             image_np[:, :, c] = channel - mean\n",
    "    #     return image_np\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the preprocessed image (assumed to already have the desired size and channels)\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        \n",
    "        # Optionally apply additional transformations if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            # Convert image to a numpy array and then to a torch tensor with shape (C, H, W)\n",
    "            image_np = np.array(image)\n",
    "            image = torch.from_numpy(image_np.transpose(2, 0, 1))\n",
    "        \n",
    "        return image, self.output_classes[idx]\n",
    "\n",
    "\n",
    "    def take_out_items(self, ratio):\n",
    "\n",
    "        indices_label0 = [i for i, label in enumerate(self.output_classes) if label == 0]\n",
    "        indices_label1 = [i for i, label in enumerate(self.output_classes) if label == 1]\n",
    "        indices_label2 = [i for i, label in enumerate(self.output_classes) if label == 2]\n",
    "        indices_label3 = [i for i, label in enumerate(self.output_classes) if label == 3]\n",
    "\n",
    "        num_to_remove_0 = math.floor(len(indices_label0) * ratio)\n",
    "        num_to_remove_1 = math.floor(len(indices_label1) * ratio)\n",
    "        num_to_remove_2 = math.floor(len(indices_label2) * ratio)\n",
    "        num_to_remove_3 = math.floor(len(indices_label3) * ratio)\n",
    "\n",
    "        selected_indices_0 = random.sample(indices_label0, num_to_remove_0) if num_to_remove_0 > 0 else []\n",
    "        selected_indices_1 = random.sample(indices_label1, num_to_remove_1) if num_to_remove_1 > 0 else []\n",
    "        selected_indices_2 = random.sample(indices_label2, num_to_remove_2) if num_to_remove_2 > 0 else []\n",
    "        selected_indices_3 = random.sample(indices_label3, num_to_remove_3) if num_to_remove_3 > 0 else []\n",
    "\n",
    "        selected_indices = set(selected_indices_0 + selected_indices_1 + selected_indices_2 + selected_indices_3)\n",
    "\n",
    "        removed_image_paths = [self.image_paths[i] for i in selected_indices]\n",
    "        removed_labels = [self.output_classes[i] for i in selected_indices]\n",
    "\n",
    "        new_image_paths = []\n",
    "        new_output_classes = []\n",
    "        for idx, (img_path, label) in enumerate(zip(self.image_paths, self.output_classes)):\n",
    "            if idx not in selected_indices:\n",
    "                new_image_paths.append(img_path)\n",
    "                new_output_classes.append(label)\n",
    "\n",
    "        self.image_paths = new_image_paths\n",
    "        self.output_classes = new_output_classes\n",
    "\n",
    "        return removed_image_paths, removed_labels\n",
    "\n",
    "    def add_items(self, list_of_image_paths,list_of_labels):\n",
    "        self.image_paths.extend(list_of_image_paths)\n",
    "        self.output_classes.extend(list_of_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ec8ad5",
   "metadata": {
    "papermill": {
     "duration": 0.002719,
     "end_time": "2025-04-07T16:12:59.778406",
     "exception": false,
     "start_time": "2025-04-07T16:12:59.775687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1faafa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T16:12:59.784967Z",
     "iopub.status.busy": "2025-04-07T16:12:59.784734Z",
     "iopub.status.idle": "2025-04-07T16:12:59.973681Z",
     "shell.execute_reply": "2025-04-07T16:12:59.972852Z"
    },
    "papermill": {
     "duration": 0.193803,
     "end_time": "2025-04-07T16:12:59.975135",
     "exception": false,
     "start_time": "2025-04-07T16:12:59.781332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import copy\n",
    "\n",
    "# ====================== HOMOMORPHIC ENCRYPTION SIMULATION ======================\n",
    "# Advanced encryption parameters\n",
    "secret_key = 3.14159\n",
    "offset = 42.0\n",
    "\n",
    "def encrypt_tensor(tensor):\n",
    "    \"\"\"\n",
    "    Advanced simulated encryption for a tensor.\n",
    "    Applies a scaling factor (secret_key) and adds an offset.\n",
    "    \"\"\"\n",
    "    return tensor * secret_key + offset\n",
    "\n",
    "def decrypt_tensor(tensor):\n",
    "    \"\"\"\n",
    "    Advanced simulated decryption for a tensor.\n",
    "    Subtracts the offset and divides by the scaling factor (secret_key)\n",
    "    to recover the original tensor.\n",
    "    \"\"\"\n",
    "    return (tensor - offset) / secret_key\n",
    "\n",
    "\n",
    "def encrypt_weights(weights_dict):\n",
    "    encrypted_dict = {}\n",
    "    for key, tensor in weights_dict.items():\n",
    "        encrypted_dict[key] = encrypt_tensor(tensor)\n",
    "    return encrypted_dict\n",
    "\n",
    "def decrypt_weights(encrypted_dict):\n",
    "    decrypted_dict = {}\n",
    "    for key, tensor in encrypted_dict.items():\n",
    "        decrypted_dict[key] = decrypt_tensor(tensor)\n",
    "    return decrypted_dict\n",
    "\n",
    "# ====================== METRICS & PLOTTING ======================\n",
    "def test_global_model(global_model, test_dataloader):\n",
    "    \"\"\"Returns test loss, accuracy, precision, recall, F1\"\"\"\n",
    "    global_model.eval()\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (images, labels) in enumerate(test_dataloader):\n",
    "            outputs = global_model(images)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(test_dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    # print(\"The Accuracy of local model after training: \",accuracy)\n",
    "    return avg_loss, accuracy, precision, recall, f1\n",
    "\n",
    "def plot_learning_curves(metrics_history, hyperparams):\n",
    "    \"\"\"Plots metrics vs communication rounds for a single hyperparameter configuration\"\"\"\n",
    "    rounds = list(range(1, len(metrics_history['loss'])+1))\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.suptitle(f\"Test Ratio: {hyperparams['test_ratio']} | Batch Size: {hyperparams['batch_size']}\\n\"\n",
    "                 f\"Rounds: {hyperparams['num_rounds']} | Epochs: {hyperparams['num_epochs']} | LR: {hyperparams['learning_rate']}\", \n",
    "                 y=1.02)\n",
    "    \n",
    "    metrics = ['loss', 'accuracy', 'precision', 'recall', 'f1']\n",
    "    titles = ['Loss', 'Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    \n",
    "    for i, (metric, title) in enumerate(zip(metrics, titles), 1):\n",
    "        plt.subplot(2, 3, i)\n",
    "        plt.plot(rounds, metrics_history[metric], marker='o')\n",
    "        plt.xlabel('Communication Round')\n",
    "        plt.ylabel(title)\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"/kaggle/working/learning_curves_ratio_{hyperparams['test_ratio']}_bs_{hyperparams['batch_size']}_rounds_{hyperparams['num_rounds']}_epochs_{hyperparams['num_epochs']}_lr_{hyperparams['learning_rate']}.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_hyperparameter_comparisons(all_results):\n",
    "    \"\"\"Generates plots comparing metrics across different hyperparameters\"\"\"\n",
    "    hyperparams = ['test_ratio', 'batch_size', 'num_rounds', 'num_epochs', 'learning_rate']\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1', 'loss']\n",
    "    \n",
    "    for hp in hyperparams:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for metric in metrics:\n",
    "            x = [res['hyperparams'][hp] for res in all_results]\n",
    "            y = [res['final_'+metric] for res in all_results]\n",
    "            plt.scatter(x, y, label=metric, alpha=0.6)\n",
    "        \n",
    "        plt.xlabel(hp)\n",
    "        plt.ylabel('Metric Value')\n",
    "        plt.title(f'Impact of {hp} on Metrics')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f\"/kaggle/working/hyperparam_{hp}_comparison.png\")\n",
    "        plt.close()\n",
    "\n",
    "# ====================== FEDERATED LEARNING CORE ======================\n",
    "def train_local_model(global_model, num_epochs, train_dataloader, test_dataloader, learning_rate, optimizer):\n",
    "    print(\"############### LOCAL MODEL LOGS ############\")\n",
    "    local_model = copy.deepcopy(global_model)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=0.001, \n",
    "        steps_per_epoch=len(train_dataloader), epochs=num_epochs\n",
    "    )\n",
    "    local_model.train()\n",
    "    for _ in range(num_epochs):\n",
    "        for images, labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = local_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_loss, val_acc, _, _, _ = test_global_model(local_model, test_dataloader)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    avg_loss, accuracy, precision, recall, f1 = test_global_model(local_model, test_dataloader)\n",
    "    print(\"After_Training Local Model:\")\n",
    "    print(\"avg_loss: \", avg_loss)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"precision: \", precision)\n",
    "    print(\"recall: \", recall)\n",
    "    \n",
    "    # Encrypt the local model weights before sharing with the global model\n",
    "    return encrypt_weights(local_model.state_dict())\n",
    "\n",
    "def federated_learning_algo(model, train_dataloaders, test_dataloader, num_rounds, num_epochs, learning_rate, hyperparams):\n",
    "    global_model = copy.deepcopy(model)\n",
    "    total_clients = len(train_dataloaders)\n",
    "    total_batches_across_dataloaders = sum([len(train_dataloader) for train_dataloader in train_dataloaders])\n",
    "    print(\"Total Number of batches across dataloaders: \", total_batches_across_dataloaders)\n",
    "    metrics_history = {'loss': [], 'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "    \n",
    "    print(f\"\\n=== Starting Training with: Test Ratio={hyperparams['test_ratio']}, Batch Size={hyperparams['batch_size']}, \"\n",
    "          f\"Rounds={num_rounds}, Epochs={num_epochs}, LR={learning_rate} ===\")\n",
    "\n",
    "    client_optimizers = [\n",
    "        torch.optim.Adam(global_model.parameters(), lr=learning_rate)\n",
    "        for _ in range(len(train_dataloaders))\n",
    "    ]\n",
    "    \n",
    "    for round in range(1, num_rounds+1):\n",
    "        print(f\"\\nRound {round}/{num_rounds}:\")\n",
    "        \n",
    "        # Local Training\n",
    "        client_weights = []\n",
    "        for client_id, (dataloader, optimizer) in enumerate(zip(train_dataloaders, client_optimizers), 1):\n",
    "            print(f\"  Client {client_id} training...\", end='\\r')\n",
    "            weights = train_local_model(global_model, num_epochs, dataloader, test_dataloader, learning_rate, optimizer)\n",
    "            client_weights.append(weights)\n",
    "        \n",
    "        # Aggregation (Federated Averaging) with homomorphic encryption simulation\n",
    "        global_weights = {}\n",
    "        for key in client_weights[0].keys():\n",
    "            for client, train_dataloader in zip(client_weights, train_dataloaders):\n",
    "                weighted_update = (len(train_dataloader) / total_batches_across_dataloaders) * client[key]\n",
    "                if key not in global_weights:\n",
    "                    global_weights[key] = weighted_update\n",
    "                else:\n",
    "                    global_weights[key] += weighted_update\n",
    "                    \n",
    "        # Decrypt aggregated weights before updating the global model\n",
    "        global_weights = decrypt_weights(global_weights)\n",
    "        global_model.load_state_dict(global_weights)\n",
    "        \n",
    "        # Testing\n",
    "        test_loss, acc, prec, rec, f1 = test_global_model(global_model, test_dataloader)\n",
    "        metrics_history['loss'].append(test_loss)\n",
    "        metrics_history['accuracy'].append(acc)\n",
    "        metrics_history['precision'].append(prec)\n",
    "        metrics_history['recall'].append(rec)\n",
    "        metrics_history['f1'].append(f1)\n",
    "\n",
    "        print(\"############## GLOBAL MODEL LOGS ##############\")\n",
    "        print(f\"  Round {round} Metrics - Loss: {test_loss:.4f} | Accuracy: {acc:.4f} | \"\n",
    "              f\"Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}       \")\n",
    "    \n",
    "    # Save learning curves for this configuration\n",
    "    plot_learning_curves(metrics_history, hyperparams)\n",
    "\n",
    "    torch.save(global_model, \"/kaggle/working/\" +\n",
    "               f\"{hyperparams['test_ratio']}_{hyperparams['batch_size']}_{hyperparams['num_rounds']}_{hyperparams['num_epochs']}_{hyperparams['learning_rate']}.pth\")\n",
    "    torch.save(global_model.state_dict(),\"/kaggle/working/\" +\n",
    "               f\"{hyperparams['test_ratio']}_{hyperparams['batch_size']}_{hyperparams['num_rounds']}_{hyperparams['num_epochs']}_{hyperparams['learning_rate']}_state_dict.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77670096",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T16:12:59.982178Z",
     "iopub.status.busy": "2025-04-07T16:12:59.981937Z",
     "iopub.status.idle": "2025-04-07T17:18:52.574957Z",
     "shell.execute_reply": "2025-04-07T17:18:52.573627Z"
    },
    "papermill": {
     "duration": 3952.598403,
     "end_time": "2025-04-07T17:18:52.576870",
     "exception": true,
     "start_time": "2025-04-07T16:12:59.978467",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of batches across dataloaders:  32\n",
      "\n",
      "=== Starting Training with: Test Ratio=0.2, Batch Size=100, Rounds=3, Epochs=1, LR=0.001 ===\n",
      "\n",
      "Round 1/3:\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After_Training Local Model:\n",
      "avg_loss:  1.4966036528348923\n",
      "Accuracy:  0.25\n",
      "precision:  0.0625\n",
      "recall:  0.25\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After_Training Local Model:\n",
      "avg_loss:  1.497248187661171\n",
      "Accuracy:  0.25\n",
      "precision:  0.0625\n",
      "recall:  0.25\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After_Training Local Model:\n",
      "avg_loss:  1.497267797589302\n",
      "Accuracy:  0.25\n",
      "precision:  0.0625\n",
      "recall:  0.25\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After_Training Local Model:\n",
      "avg_loss:  1.4958789497613907\n",
      "Accuracy:  0.25\n",
      "precision:  0.0625\n",
      "recall:  0.25\n",
      "############## GLOBAL MODEL LOGS ##############\n",
      "  Round 1 Metrics - Loss: 1.4967 | Accuracy: 0.2500 | Precision: 0.0625 | Recall: 0.2500 | F1: 0.1000       \n",
      "\n",
      "Round 2/3:\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After_Training Local Model:\n",
      "avg_loss:  1.4662956893444061\n",
      "Accuracy:  0.25\n",
      "precision:  0.0625\n",
      "recall:  0.25\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After_Training Local Model:\n",
      "avg_loss:  1.4683493226766586\n",
      "Accuracy:  0.25\n",
      "precision:  0.0625\n",
      "recall:  0.25\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After_Training Local Model:\n",
      "avg_loss:  1.4664427191019058\n",
      "Accuracy:  0.25\n",
      "precision:  0.0625\n",
      "recall:  0.25\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After_Training Local Model:\n",
      "avg_loss:  1.466157466173172\n",
      "Accuracy:  0.25\n",
      "precision:  0.0625\n",
      "recall:  0.25\n",
      "############## GLOBAL MODEL LOGS ##############\n",
      "  Round 2 Metrics - Loss: 1.4668 | Accuracy: 0.2500 | Precision: 0.0625 | Recall: 0.2500 | F1: 0.1000       \n",
      "\n",
      "Round 3/3:\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After_Training Local Model:\n",
      "avg_loss:  1.4678419530391693\n",
      "Accuracy:  0.25\n",
      "precision:  0.0625\n",
      "recall:  0.25\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After_Training Local Model:\n",
      "avg_loss:  1.4700976461172104\n",
      "Accuracy:  0.25\n",
      "precision:  0.0625\n",
      "recall:  0.25\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After_Training Local Model:\n",
      "avg_loss:  1.4685321003198624\n",
      "Accuracy:  0.25\n",
      "precision:  0.0625\n",
      "recall:  0.25\n",
      "############### LOCAL MODEL LOGS ############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After_Training Local Model:\n",
      "avg_loss:  1.4681012481451035\n",
      "Accuracy:  0.25\n",
      "precision:  0.0625\n",
      "recall:  0.25\n",
      "############## GLOBAL MODEL LOGS ##############\n",
      "  Round 3 Metrics - Loss: 1.4686 | Accuracy: 0.2500 | Precision: 0.0625 | Recall: 0.2500 | F1: 0.1000       \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-34f101146756>\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# Generate hyperparameter comparison plots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mplot_hyperparameter_comparisons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-ee08a0db846c>\u001b[0m in \u001b[0;36mplot_hyperparameter_comparisons\u001b[0;34m(all_results)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hyperparams'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'final_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-ee08a0db846c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hyperparams'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'final_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assume model and datasets are predefined\n",
    "model = sample_CNN_model()  # Your model definition\n",
    "\n",
    "# For training data (with augmentations)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((320, 320)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# For validation/test data (no augmentations)\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((320, 320)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "center_datasets = [custom_dataset(folder_path=\"/kaggle/input/new-centers/Center_1_zipp/Center_1\",transform=train_transforms),custom_dataset(folder_path=\"/kaggle/input/new-centers/Center_2_zipp/Center_2\",transform=train_transforms), custom_dataset(folder_path=\"/kaggle/input/new-centers/Center_3_zipp/Center_3\",transform=train_transforms), custom_dataset(folder_path=\"/kaggle/input/new-centers/Center_4_zipp/Center_4\",transform=train_transforms)]  # Your central datasets\n",
    "\n",
    "# Hyperparameter Search Space\n",
    "test_ratios = [0.2]\n",
    "batch_sizes = [100]\n",
    "num_rounds_list = [3]\n",
    "num_epochs_list = [1]\n",
    "learning_rates = [0.001] #,0.00007,0.0001,0.0007\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for test_ratio in test_ratios:\n",
    "    \n",
    "    # Dataset preparation\n",
    "    to_put_image_paths=[]\n",
    "    to_put_image_labels=[]\n",
    "    for center_dataset in center_datasets:\n",
    "        to_append_image_paths,to_append_image_labels=center_dataset.take_out_items(test_ratio)\n",
    "        to_put_image_paths.extend(to_append_image_paths)\n",
    "        to_put_image_labels.extend(to_append_image_labels)\n",
    "    test_dataset=custom_dataset(transform=test_transforms)#Here the folder path is None so that it makes an empty dataset\n",
    "    test_dataset.add_items(to_put_image_paths,to_put_image_labels)\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        train_dataloaders = [DataLoader(center_dataset, batch_size=batch_size,shuffle=True) for center_dataset in center_datasets]\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size,shuffle=True)\n",
    "        \n",
    "        for num_rounds in num_rounds_list:\n",
    "            for num_epochs in num_epochs_list:\n",
    "                for lr in learning_rates:\n",
    "                    hyperparams = {\n",
    "                        'test_ratio': test_ratio,\n",
    "                        'batch_size': batch_size,\n",
    "                        'num_rounds': num_rounds,\n",
    "                        'num_epochs': num_epochs,\n",
    "                        'learning_rate': lr\n",
    "                    }\n",
    "                    \n",
    "                    # Run federated learning\n",
    "                    result = federated_learning_algo(\n",
    "                        model, train_dataloaders, test_dataloader, \n",
    "                        num_rounds, num_epochs, lr, hyperparams\n",
    "                    )\n",
    "                    all_results.append(result)\n",
    "                    \n",
    "\n",
    "# Generate hyperparameter comparison plots\n",
    "plot_hyperparameter_comparisons(all_results)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7046228,
     "sourceId": 11271844,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7076025,
     "sourceId": 11313209,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3978.589902,
   "end_time": "2025-04-07T17:18:54.311794",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-07T16:12:35.721892",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
